# spark-
大数据中最棘手的问题--数据倾斜，此时spark的性能会比期望差很多，数据倾斜调优，就是使用各种技术方案解决不同类型的数据倾斜问题，以保证Spark作业的性能。

数据倾斜发生的现象：
1.比如有1000个task，其中997个task都花了几分钟运行完了，最后3个task确花了几个小时才跑完。
2.正常执行中的spark作业，突然报OOM（内存溢出）的错误，观察异常栈，却是我们的代码出现问题。
